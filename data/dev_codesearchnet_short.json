[
 {
  "idx": 1,
  "doc": "Helper which expand_dims is_accepted then applies tf . where .",
  "code": "def choose ( is_accepted , accepted , rejected , name = None ) : if not is_namedtuple_like ( accepted ) : return _choose_base_case ( is_accepted , accepted , rejected , name = name ) if not isinstance ( accepted , type ( rejected ) ) : raise TypeError ( 'Type of `accepted` ({}) must be identical to ' 'type of `rejected` ({})' . format ( type ( accepted ) . __name__ , type ( rejected ) . __name__ ) ) return type ( accepted ) ( * * dict ( [ ( fn , choose ( is_accepted , getattr ( accepted , fn ) , getattr ( rejected , fn ) , name = name ) ) for fn in accepted . _fields ] ) )",
  "label": 1
 },
 {
  "idx": 2,
  "doc": "Is event consistent with the given evidence?",
  "code": "def consistent_with ( event , evidence ) : return every ( lambda ( k , v ) : evidence . get ( k , v ) == v , event . items ( ) )",
  "label": 1
 },
 {
  "idx": 3,
  "doc": "compute similarities in appended files",
  "code": "def _compute_sims ( self ) : no_duplicates = defaultdict ( list ) for num , lineset1 , idx1 , lineset2 , idx2 in self . _iter_sims ( ) : duplicate = no_duplicates [ num ] for couples in duplicate : if ( lineset1 , idx1 ) in couples or ( lineset2 , idx2 ) in couples : couples . add ( ( lineset1 , idx1 ) ) couples . add ( ( lineset2 , idx2 ) ) break else : duplicate . append ( { ( lineset1 , idx1 ) , ( lineset2 , idx2 ) } ) sims = [ ] for num , ensembles in no_duplicates . items ( ) : for couples in ensembles : sims . append ( ( num , couples ) ) sims . sort ( ) sims . reverse ( ) return sims",
  "label": 1
 },
 {
  "idx": 4,
  "doc": "Print a status message about the logger .",
  "code": "def logstate ( self ) : if self . logfile is None : print 'Logging has not been activated.' else : state = self . log_active and 'active' or 'temporarily suspended' print 'Filename       :' , self . logfname print 'Mode           :' , self . logmode print 'Output logging :' , self . log_output print 'Raw input log  :' , self . log_raw_input print 'Timestamping   :' , self . timestamp print 'State          :' , state",
  "label": 1
 },
 {
  "idx": 5,
  "doc": "Cancel all started queries that have not yet completed",
  "code": "def cancel_query ( self ) : jobs = self . service . jobs ( ) if ( self . running_job_id and not self . poll_job_complete ( self . running_job_id ) ) : self . log . info ( 'Attempting to cancel job : %s, %s' , self . project_id , self . running_job_id ) if self . location : jobs . cancel ( projectId = self . project_id , jobId = self . running_job_id , location = self . location ) . execute ( num_retries = self . num_retries ) else : jobs . cancel ( projectId = self . project_id , jobId = self . running_job_id ) . execute ( num_retries = self . num_retries ) else : self . log . info ( 'No running BigQuery jobs to cancel.' ) return # Wait for all the calls to cancel to finish max_polling_attempts = 12 polling_attempts = 0 job_complete = False while polling_attempts < max_polling_attempts and not job_complete : polling_attempts = polling_attempts + 1 job_complete = self . poll_job_complete ( self . running_job_id ) if job_complete : self . log . info ( 'Job successfully canceled: %s, %s' , self . project_id , self . running_job_id ) elif polling_attempts == max_polling_attempts : self . log . info ( \"Stopping polling due to timeout. Job with id %s \" \"has not completed cancel and may or may not finish.\" , self . running_job_id ) else : self . log . info ( 'Waiting for canceled job with id %s to finish.' , self . running_job_id ) time . sleep ( 5 )",
  "label": 1
 },
 {
  "idx": 6,
  "doc": "Fill the entire strip with RGB color tuple",
  "code": "def fill ( self , color , start = 0 , end = - 1 ) : start = max ( start , 0 ) if end < 0 or end >= self . numLEDs : end = self . numLEDs - 1 for led in range ( start , end + 1 ) : # since 0-index include end in range self . _set_base ( led , color )",
  "label": 1
 },
 {
  "idx": 7,
  "doc": "Generates a set of input record",
  "code": "def generateRandomInput ( numRecords , elemSize = 400 , numSet = 42 ) : inputs = [ ] for _ in xrange ( numRecords ) : input = np . zeros ( elemSize , dtype = realDType ) for _ in range ( 0 , numSet ) : ind = np . random . random_integers ( 0 , elemSize - 1 , 1 ) [ 0 ] input [ ind ] = 1 while abs ( input . sum ( ) - numSet ) > 0.1 : ind = np . random . random_integers ( 0 , elemSize - 1 , 1 ) [ 0 ] input [ ind ] = 1 inputs . append ( input ) return inputs",
  "label": 1
 },
 {
  "idx": 8,
  "doc": "Perform an action on the thing .",
  "code": "def perform_action ( self , action_name , input_ = None ) : if action_name not in self . available_actions : return None action_type = self . available_actions [ action_name ] if 'input' in action_type [ 'metadata' ] : try : validate ( input_ , action_type [ 'metadata' ] [ 'input' ] ) except ValidationError : return None action = action_type [ 'class' ] ( self , input_ = input_ ) action . set_href_prefix ( self . href_prefix ) self . action_notify ( action ) self . actions [ action_name ] . append ( action ) return action",
  "label": 1
 },
 {
  "idx": 9,
  "doc": "Remove memory of state variables set in the command processor",
  "code": "def forget ( self ) : self . stack = [ ] self . curindex = 0 self . curframe = None self . thread_name = None self . frame_thread_name = None return",
  "label": 1
 },
 {
  "idx": 10,
  "doc": "Split a covariance matrix into block - diagonal marginals of given sizes .",
  "code": "def _split_covariance_into_marginals ( covariance , block_sizes ) : start_dim = 0 marginals = [ ] for size in block_sizes : end_dim = start_dim + size marginals . append ( covariance [ ... , start_dim : end_dim , start_dim : end_dim ] ) start_dim = end_dim return marginals",
  "label": 1
 },
 {
  "idx": 11,
  "doc": "Hydrate Generated Python AST nodes with line numbers and column offsets if they exist in the node environment .",
  "code": "def _ast_with_loc ( py_ast : GeneratedPyAST , env : NodeEnv , include_dependencies : bool = False ) -> GeneratedPyAST : if env . line is not None : py_ast . node . lineno = env . line if include_dependencies : for dep in py_ast . dependencies : dep . lineno = env . line if env . col is not None : py_ast . node . col_offset = env . col if include_dependencies : for dep in py_ast . dependencies : dep . col_offset = env . col return py_ast",
  "label": 1
 },
 {
  "idx": 12,
  "doc": "Return the entire source file and starting line number for an object .",
  "code": "def findsource ( object ) : file = getsourcefile ( object ) or getfile ( object ) # If the object is a frame, then trying to get the globals dict from its # module won't work. Instead, the frame object itself has the globals # dictionary. globals_dict = None if inspect . isframe ( object ) : # XXX: can this ever be false? globals_dict = object . f_globals else : module = getmodule ( object , file ) if module : globals_dict = module . __dict__ lines = linecache . getlines ( file , globals_dict ) if not lines : raise IOError ( 'could not get source code' ) if ismodule ( object ) : return lines , 0 if isclass ( object ) : name = object . __name__ pat = re . compile ( r'^(\\s*)class\\s*' + name + r'\\b' ) # make some effort to find the best matching class definition: # use the one with the least indentation, which is the one # that's most probably not inside a function definition. candidates = [ ] for i in range ( len ( lines ) ) : match = pat . match ( lines [ i ] ) if match : # if it's at toplevel, it's already the best one if lines [ i ] [ 0 ] == 'c' : return lines , i # else add whitespace to candidate list candidates . append ( ( match . group ( 1 ) , i ) ) if candidates : # this will sort by whitespace, and by line number, # less whitespace first candidates . sort ( ) return lines , candidates [ 0 ] [ 1 ] else : raise IOError ( 'could not find class definition' ) if ismethod ( object ) : object = object . im_func if isfunction ( object ) : object = object . func_code if istraceback ( object ) : object = object . tb_frame if isframe ( object ) : object = object . f_code if iscode ( object ) : if not hasattr ( object , 'co_firstlineno' ) : raise IOError ( 'could not find function definition' ) pat = re . compile ( r'^(\\s*def\\s)|(.*(?<!\\w)lambda(:|\\s))|^(\\s*@)' ) pmatch = pat . match # fperez - fix: sometimes, co_firstlineno can give a number larger than # the length of lines, which causes an error.  Safeguard against that. lnum = min ( object . co_firstlineno , len ( lines ) ) - 1 while lnum > 0 : if pmatch ( lines [ lnum ] ) : break lnum -= 1 return lines , lnum raise IOError ( 'could not find code object' )",
  "label": 1
 },
 {
  "idx": 13,
  "doc": "Return a dictionary consisting of the key itself",
  "code": "def get ( self , key_name ) : result = self . db . search ( Query ( ) . name == key_name ) if not result : return { } return result [ 0 ]",
  "label": 1
 },
 {
  "idx": 14,
  "doc": "Read the user configuration",
  "code": "def _read_config ( self , filename = None ) : if filename : self . _config_filename = filename else : try : import appdirs except ImportError : raise Exception ( \"Missing dependency for determining config path. Please install \" \"the 'appdirs' Python module.\" ) self . _config_filename = appdirs . user_config_dir ( _LIBRARY_NAME , \"ProfitBricks\" ) + \".ini\" if not self . _config : self . _config = configparser . ConfigParser ( ) self . _config . optionxform = str self . _config . read ( self . _config_filename )",
  "label": 1
 },
 {
  "idx": 15,
  "doc": "Compare vectors . Borrowed from A . Parish .",
  "code": "def cosine ( vec1 , vec2 ) : if norm ( vec1 ) > 0 and norm ( vec2 ) > 0 : return dot ( vec1 , vec2 ) / ( norm ( vec1 ) * norm ( vec2 ) ) else : return 0.0",
  "label": 1
 },
 {
  "idx": 16,
  "doc": "Retrieve an estimated time correction offset for the given stream .",
  "code": "def time_correction ( self , timeout = FOREVER ) : errcode = c_int ( ) result = lib . lsl_time_correction ( self . obj , c_double ( timeout ) , byref ( errcode ) ) handle_error ( errcode ) return result",
  "label": 1
 },
 {
  "idx": 17,
  "doc": "This will output the nginx HTTP config string for specific port spec",
  "code": "def _nginx_http_spec ( port_spec , bridge_ip ) : server_string_spec = \"\\t server {\\n\" server_string_spec += \"\\t \\t {}\\n\" . format ( _nginx_max_file_size_string ( ) ) server_string_spec += \"\\t \\t {}\\n\" . format ( _nginx_listen_string ( port_spec ) ) server_string_spec += \"\\t \\t {}\\n\" . format ( _nginx_server_name_string ( port_spec ) ) server_string_spec += _nginx_location_spec ( port_spec , bridge_ip ) server_string_spec += _custom_502_page ( ) server_string_spec += \"\\t }\\n\" return server_string_spec",
  "label": 1
 },
 {
  "idx": 18,
  "doc": "Create a callable that will invoke the given remote function . The stub will return a deferred even if the remote function does not .",
  "code": "def create_function_stub ( self , url ) : assert self . _opened , \"RPC System is not opened\" logging . debug ( \"create_function_stub(%s)\" % repr ( url ) ) parseresult = urlparse . urlparse ( url ) scheme = parseresult . scheme path = parseresult . path . split ( \"/\" ) if scheme != \"anycall\" : raise ValueError ( \"Not an anycall URL: %s\" % repr ( url ) ) if len ( path ) != 3 or path [ 0 ] != \"\" or path [ 1 ] != \"functions\" : raise ValueError ( \"Not an URL for a remote function: %s\" % repr ( url ) ) try : functionid = uuid . UUID ( path [ 2 ] ) except ValueError : raise ValueError ( \"Not a valid URL for a remote function: %s\" % repr ( url ) ) return _RPCFunctionStub ( parseresult . netloc , functionid , self )",
  "label": 1
 },
 {
  "idx": 19,
  "doc": "Given a frame and a compiled function code find the corresponding function object within the frame .",
  "code": "def _find_function_from_code ( frame , code ) : def find_code ( iterable , depth = 0 ) : if depth > 3 : return # Avoid potential infinite loops, or generally objects that are too deep. for item in iterable : if item is None : continue found = None if hasattr ( item , \"__code__\" ) and item . __code__ == code : found = item elif isinstance ( item , type ) or isinstance ( item , ModuleType ) : # class / module try : found = find_code ( ( getattr ( item , n , None ) for n in dir ( item ) ) , depth + 1 ) except Exception : # Sometimes merely getting module's attributes may cause an exception. For example :mod:`six.moves` # is such an offender... continue elif isinstance ( item , ( list , tuple , set ) ) : found = find_code ( item , depth + 1 ) elif isinstance ( item , dict ) : found = find_code ( item . values ( ) , depth + 1 ) if found : return found return find_code ( frame . f_locals . values ( ) ) or find_code ( frame . f_globals . values ( ) )",
  "label": 1
 },
 {
  "idx": 20,
  "doc": "Convert unixtime to unixtime on GTFS start - of - day .",
  "code": "def day_start_ut ( self , ut ) : # set timezone to the one of gtfs old_tz = self . set_current_process_time_zone ( ) ut = time . mktime ( time . localtime ( ut ) [ : 3 ] + ( 12 , 00 , 0 , 0 , 0 , - 1 ) ) - 43200 set_process_timezone ( old_tz ) return ut",
  "label": 1
 },
 {
  "idx": 21,
  "doc": "Use this if you want to clone an existing contact and replace its data with new user input in one step .",
  "code": "def from_existing_contact_with_new_user_input ( cls , contact , user_input , localize_dates ) : contact = cls ( contact . address_book , contact . filename , contact . supported_private_objects , None , localize_dates ) contact . _process_user_input ( user_input ) return contact",
  "label": 1
 },
 {
  "idx": 22,
  "doc": "Associate a notification template from this workflow .",
  "code": "def associate_notification_template ( self , workflow , notification_template , status ) : return self . _assoc ( 'notification_templates_%s' % status , workflow , notification_template )",
  "label": 1
 },
 {
  "idx": 23,
  "doc": "L1 and L2 rules",
  "code": "def reorder_resolved_levels ( storage , debug ) : # Applies L1. should_reset = True chars = storage [ 'chars' ] for _ch in chars [ : : - 1 ] : # L1. On each line, reset the embedding level of the following # characters to the paragraph embedding level: if _ch [ 'orig' ] in ( 'B' , 'S' ) : # 1. Segment separators, # 2. Paragraph separators, _ch [ 'level' ] = storage [ 'base_level' ] should_reset = True elif should_reset and _ch [ 'orig' ] in ( 'BN' , 'WS' ) : # 3. Any sequence of whitespace characters preceding a segment # separator or paragraph separator # 4. Any sequence of white space characters at the end of the # line. _ch [ 'level' ] = storage [ 'base_level' ] else : should_reset = False max_len = len ( chars ) # L2 should be per line # Calculates highest level and loweset odd level on the fly. line_start = line_end = 0 highest_level = 0 lowest_odd_level = EXPLICIT_LEVEL_LIMIT for idx in range ( max_len ) : _ch = chars [ idx ] # calc the levels char_level = _ch [ 'level' ] if char_level > highest_level : highest_level = char_level if char_level % 2 and char_level < lowest_odd_level : lowest_odd_level = char_level if _ch [ 'orig' ] == 'B' or idx == max_len - 1 : line_end = idx # omit line breaks if _ch [ 'orig' ] == 'B' : line_end -= 1 reverse_contiguous_sequence ( chars , line_start , line_end , highest_level , lowest_odd_level ) # reset for next line run line_start = idx + 1 highest_level = 0 lowest_odd_level = EXPLICIT_LEVEL_LIMIT if debug : debug_storage ( storage )",
  "label": 1
 },
 {
  "idx": 24,
  "doc": "Find a webhook by name .",
  "code": "def delete_webhooks_with_name ( api , name ) : for webhook in api . webhooks . list ( ) : if webhook . name == name : print ( \"Deleting Webhook:\" , webhook . name , webhook . targetUrl ) api . webhooks . delete ( webhook . id )",
  "label": 1
 },
 {
  "idx": 25,
  "doc": "Split the extension from a pathname .",
  "code": "def _splitext ( p , sep , altsep , extsep ) : sepIndex = p . rfind ( sep ) if altsep : altsepIndex = p . rfind ( altsep ) sepIndex = max ( sepIndex , altsepIndex ) dotIndex = p . rfind ( extsep ) if dotIndex > sepIndex : # skip all leading dots filenameIndex = sepIndex + 1 while filenameIndex < dotIndex : if p [ filenameIndex ] != extsep : return p [ : dotIndex ] , p [ dotIndex : ] filenameIndex += 1 return p , ''",
  "label": 1
 }
]
